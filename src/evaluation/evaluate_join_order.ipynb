{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from classes.classes import ColorManager\n",
    "from classes.classes import E2EModelConfig\n",
    "from classes.classes import FlatModelActCardModelConfig, QPPModelActCardsConfig, ZeroShotModelActCardConfig, \\\n",
    "    DACEModelActCardConfig, DACEModelConfig, FlatModelConfig\n",
    "from classes.classes import MODEL_CONFIGS\n",
    "from classes.classes import ZeroShotModelConfig, QPPNetModelConfig, ScaledPostgresModelConfig\n",
    "from classes.paths import LocalPaths\n",
    "from classes.workloads import EvalWorkloads\n",
    "from classes.workloads import JoinOrderEvalWorkload\n",
    "from cross_db_benchmark.datasets.datasets import Database\n",
    "from evaluation.evaluation_metrics import QError\n",
    "from evaluation.evaluation_metrics import SelectedRuntime\n",
    "from evaluation.evaluation_metrics import SpearmanCorrelation\n",
    "from evaluation.utils import draw_bushiness\n",
    "from evaluation.utils import get_model_results, draw_predictions, draw_metric\n",
    "from evaluation.evaluation_metrics import MissedPlansFraction, MaxOverestimation, MaxUnderestimation\n",
    "from classes.classes import ACT_CARD_MODEL_CONFIGS\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.8)\n",
    "fontsize = 14"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d822be8b06979e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## 2. Join Order Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a8791e35426193c"
  },
  {
   "cell_type": "code",
   "source": [
    "path = LocalPaths().data / \"plots\" / \"join_order_examples.pdf\"\n",
    "mosaic = \"\"\"AAAABBEE\n",
    "            AAAACCFF\n",
    "            AAAADDGG\"\"\"\n",
    "folder = \"join_order_full\"\n",
    "grid_spec = {'height_ratios': [1,1,1], 'wspace': 3, 'hspace': 0.3}\n",
    "\n",
    "workloads = [JoinOrderEvalWorkload(database=Database(\"imdb\"), folder=folder, wl_name=\"job_light_33\", num_tables=4)]\n",
    "\n",
    "title_1 =(\"SELECT COUNT(*) FROM title, movie_keyword, movie_companies\\n\"\n",
    "          \"WHERE title.id=movie_keyword.movie_id \"\n",
    "          \"AND title.id=movie_companies.movie_id AND title.prod_year>1950;\")\n",
    "\n",
    "fig = plt.figure(figsize=(11 * len(workloads) , 3), dpi=100)\n",
    "figures= fig.subfigures(nrows=1, ncols=len(workloads), wspace=-0.1, hspace=-0.05)\n",
    "\n",
    "for idx, (workload, figure) in enumerate(zip(workloads, [figures])):\n",
    "    subplots = figure.subplot_mosaic(mosaic, gridspec_kw=grid_spec).values()\n",
    "    [prediction, q_error, runtime, missed_plans, spearmans, underest, overest] = subplots\n",
    "    results = get_model_results(workload, MODEL_CONFIGS)\n",
    "    # sort results by real runtime\n",
    "    results = results.sort_values(by='runtime')\n",
    "    draw_predictions(workload, results, MODEL_CONFIGS, prediction, fontsize=fontsize)\n",
    "    draw_metric(results, MODEL_CONFIGS, q_error, QError(), fontsize)\n",
    "    draw_metric(results, MODEL_CONFIGS, spearmans, SpearmanCorrelation(), fontsize)\n",
    "    draw_metric(results, MODEL_CONFIGS, missed_plans, MissedPlansFraction(), fontsize)\n",
    "    draw_metric(results, MODEL_CONFIGS, overest, MaxOverestimation(), fontsize)\n",
    "    draw_metric(results, MODEL_CONFIGS, underest, MaxUnderestimation(), fontsize)\n",
    "    draw_metric(results, MODEL_CONFIGS, runtime, SelectedRuntime(display_name=\"Selected\\nRuntime(s)\"), fontsize)\n",
    "    prediction.set_xlabel('Join Enumeration', fontsize=fontsize)\n",
    "    q_error.set_ylim(1, 3)\n",
    "    q_error.set_yticks([1, 2, 3], labels=[1, 2, 3], fontsize=fontsize)\n",
    "    q_error.minorticks_off()\n",
    "    runtime.set_ylim(0, 6)\n",
    "    underest.set_yticks([1, 3, 5], labels=[1, 3, 5], fontsize=fontsize)\n",
    "    underest.minorticks_off()\n",
    "    overest.set_yticks([1, 5, 10], labels=[1, 5, 10], fontsize=fontsize)\n",
    "    overest.minorticks_off()\n",
    "\n",
    "    figure.suptitle(title_1, fontsize=fontsize*0.8, fontproperties={'family':'monospace'}, y=1.03, horizontalalignment='center')\n",
    "    runtime.axhline(y=results['runtime'].min(), linestyle='--', color='black', linewidth=2, zorder=100)\n",
    "    runtime.annotate(text='Optimal\\nRuntime',\n",
    "                     xy=(1.05, 0.21),\n",
    "                     xycoords='axes fraction',\n",
    "                     fontsize=fontsize * 0.75,\n",
    "                     ha='left',\n",
    "                     va='bottom')\n",
    "\n",
    "    for plot, letter in zip(list(subplots), \"ABCDEFG\"):\n",
    "        plot.annotate(\n",
    "            letter,\n",
    "            xy=(0.05, 0.95), \n",
    "            xycoords='axes fraction', \n",
    "            fontsize=9, \n",
    "            ha='center', \n",
    "            va='center', \n",
    "            bbox=dict(boxstyle='circle,pad=0.2', edgecolor='black', facecolor='white'))\n",
    "        \n",
    "        \n",
    "legend_handles = [mpatches.Patch(color=model_config.color(), label=model_config.name.DISPLAY_NAME) for model_config in MODEL_CONFIGS]\n",
    "for p in legend_handles:\n",
    "    p.set_edgecolor('black')\n",
    "legend_handles.append(Line2D([0], [0], color='black', lw=1, linestyle='-', label='Real Runtime'))\n",
    "\n",
    "legend = q_error.legend(handles=legend_handles, \n",
    "                        fontsize=fontsize,\n",
    "                        ncol=1, \n",
    "                        loc='center left', \n",
    "                        bbox_to_anchor=(-5, -0.8),\n",
    "                        labelspacing=0.3,\n",
    "                        edgecolor='white')\n",
    "for line in legend.get_lines():\n",
    "    line.set_linewidth(6.0)\n",
    "    \n",
    "fig.align_labels()\n",
    "plt.savefig(path, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7511b4bead663a1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results",
   "id": "59bcb12ac025ea42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from classes.classes import QueryFormerModelConfig, E2EModelConfig\n",
    "metric = SpearmanCorrelation()\n",
    "model_configs = [ScaledPostgresModelConfig(), QueryFormerModelConfig(), E2EModelConfig()]\n",
    "extract = pd.DataFrame({\n",
    "        model.name.DISPLAY_NAME: metric.evaluate_metric(preds=results[results[\"model\"] == model.name.DISPLAY_NAME][\"prediction\"],\n",
    "                                                        labels=results[results[\"model\"] == model.name.DISPLAY_NAME]['runtime']) for model in model_configs},\n",
    "        index=[metric.metric_name]).T\n",
    "extract"
   ],
   "id": "1fc04d50fcf9964f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results[\"runtime\"].max()",
   "id": "b9a81490eae25a7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## 3. Bushiness Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90f1fff74cbdb0c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path = LocalPaths().data / \"plots\" / \"join_order_bushiness.pdf\"\n",
    "workloads = [JoinOrderEvalWorkload(database=Database(\"baseball\"), folder=folder, wl_name=\"baseball_14\", num_tables=4),\n",
    "             JoinOrderEvalWorkload(database=Database(\"imdb\"), folder=folder, wl_name=\"job_light_34\", num_tables=4),\n",
    "             JoinOrderEvalWorkload(database=Database(\"tpc_h\"), folder=folder, wl_name=\"tpc_h_88\", num_tables=4),\n",
    "             JoinOrderEvalWorkload(database=Database(\"baseball\"), folder=folder, wl_name=\"baseball_58\", num_tables=4),\n",
    "             JoinOrderEvalWorkload(database=Database(\"imdb\"), folder=folder, wl_name=\"job_light_33\", num_tables=4),\n",
    "             JoinOrderEvalWorkload(database=Database(\"tpc_h\"), folder=folder, wl_name=\"tpc_h_117\", num_tables=4)]\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize=(20, 6), dpi=100)\n",
    "\n",
    "for workload, ax in zip(workloads, axs.flatten()):\n",
    "    results = get_model_results(workload, MODEL_CONFIGS)\n",
    "    draw_bushiness(results, MODEL_CONFIGS, ax)\n",
    "\n",
    "axs[0][0].set_ylabel(\"Q-Error\")\n",
    "axs[1][0].set_ylabel(\"Q-Error\")\n",
    "fig.align_labels()\n",
    "plt.savefig(path, bbox_inches='tight')"
   ],
   "id": "62dfaef916232aab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## 3. Join Order (Full Benchmark)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34214291c21c9a04"
  },
  {
   "cell_type": "code",
   "source": [
    "from classes.classes import MSCNModelConfig, QueryFormerModelConfig\n",
    "\n",
    "path = LocalPaths().data / \"plots\" / \"join_order_full.pdf\"\n",
    "\n",
    "TEST_MODEL_CONFIGS = [\n",
    "    ScaledPostgresModelConfig(),\n",
    "    FlatModelConfig(),\n",
    "    MSCNModelConfig(),\n",
    "    E2EModelConfig(),\n",
    "    ZeroShotModelConfig(),\n",
    "    QPPNetModelConfig(),\n",
    "    QueryFormerModelConfig(),\n",
    "    DACEModelConfig(),\n",
    "    FlatModelActCardModelConfig(),\n",
    "    QPPModelActCardsConfig(),\n",
    "    ZeroShotModelActCardConfig(),\n",
    "    DACEModelActCardConfig()\n",
    "]\n",
    "\n",
    "for plot, model_configs in zip([\"join_order_full\", \"join_order_full_act_cards\"], [MODEL_CONFIGS, TEST_MODEL_CONFIGS]):\n",
    "    path = LocalPaths().data / \"plots\" / f\"{plot}.pdf\"\n",
    "    \n",
    "    spearman_df = pd.DataFrame()\n",
    "    runtime_df = pd.DataFrame()\n",
    "    missed_plans_df = pd.DataFrame()\n",
    "    overest_df  = pd.DataFrame()\n",
    "    underest_df = pd.DataFrame()\n",
    "    minimal_runtimes = []\n",
    "    \n",
    "    for workload in EvalWorkloads.FullJoinOrder.imdb:\n",
    "        results = get_model_results(workload, model_configs)\n",
    "        for model in model_configs:\n",
    "            model_results = results[results[\"model\"] == model.name.DISPLAY_NAME]\n",
    "            spearman_corr = SpearmanCorrelation().evaluate_metric(preds=model_results[\"prediction\"], \n",
    "                                                                  labels=model_results['runtime'])\n",
    "            spearman_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = spearman_corr\n",
    "            selected_runtime = SelectedRuntime().evaluate_metric(preds=model_results[\"prediction\"], labels=model_results['runtime'])\n",
    "            runtime_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = selected_runtime\n",
    "            missed_plans = MissedPlansFraction().evaluate_metric(preds=model_results[\"prediction\"], labels=model_results['runtime'])\n",
    "            missed_plans_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = missed_plans\n",
    "            overest = MaxOverestimation().evaluate_metric(preds=model_results[\"prediction\"], labels=model_results['runtime'])\n",
    "            overest_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = overest\n",
    "            underest = MaxUnderestimation().evaluate_metric(preds=model_results[\"prediction\"], labels=model_results['runtime'])\n",
    "            underest_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = underest\n",
    "        minimal_runtimes.append(model_results[\"label\"].min())\n",
    "    \n",
    "    # Create the boxplot and barplot\n",
    "    fig, (ax2, ax3, ax1, ax4, ax5) = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    \n",
    "    sns.boxplot(data=spearman_df.T, palette=ColorManager.COLOR_PALETTE, ax=ax1)\n",
    "    #ax1.set_title('Spearman Correlation over JOB-Light Permutations')\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_title('Spearman Correlation', fontsize=fontsize)\n",
    "    ax1.xaxis.set_ticklabels([])\n",
    "    ax1.set_ylim(-1, 1)\n",
    "    ax1.tick_params(axis='y', which='major', pad=0, labelsize=fontsize)\n",
    "    \n",
    "    sns.barplot(data=runtime_df.sum(axis=1).reset_index(), x='index', hue='index', palette=ColorManager.COLOR_PALETTE, y=0, ax=ax2, edgecolor='black')\n",
    "    #ax2.set_title('Total Runtime for Each Model')\n",
    "    ax2.set_xlabel('')\n",
    "    ax2.set_title('Total Runtime (s)', fontsize=fontsize)\n",
    "    ax2.xaxis.set_ticklabels([])\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ax2.axhline(y=sum(minimal_runtimes), color='black', linestyle='--')\n",
    "    ax2.tick_params(axis='y', which='major', pad=0, labelsize=fontsize)\n",
    "    \n",
    "    sns.boxplot(data=missed_plans_df.T, palette=ColorManager.COLOR_PALETTE, ax=ax3)\n",
    "    ax3.set_xlabel('')\n",
    "    ax3.set_title('Surpassed Plans (%)', fontsize=fontsize)\n",
    "    ax3.xaxis.set_ticklabels([])\n",
    "    ax3.tick_params(axis='y', which='major', pad=0,  labelsize=fontsize)\n",
    "    \n",
    "    sns.boxplot(data=underest_df.T, palette=ColorManager.COLOR_PALETTE, ax=ax4)\n",
    "    ax4.set_xlabel('')\n",
    "    ax4.set_title('Underestimation', fontsize=fontsize)\n",
    "    ax4.xaxis.set_ticklabels([])\n",
    "    ax4.set_ylim(0.9, 25)\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.tick_params(axis='y', which='major', pad=0,  labelsize=fontsize)\n",
    "    \n",
    "    sns.boxplot(data=overest_df.T, palette=ColorManager.COLOR_PALETTE, ax=ax5)\n",
    "    ax5.set_xlabel('')\n",
    "    ax5.set_title('Overestimation', fontsize=fontsize)\n",
    "    ax5.xaxis.set_ticklabels([])\n",
    "    ax5.set_ylim(0.9, 25)\n",
    "    ax5.set_yscale('log')\n",
    "    ax5.tick_params(axis='y', which='major', pad=0,  labelsize=fontsize)\n",
    "    \n",
    "    for ax in (ax2, ax3, ax1, ax4, ax5):\n",
    "        ax.axvspan(xmin=7.5, xmax=11.5, alpha=0.1, color='gray')\n",
    "    \n",
    "    # Create legend patches\n",
    "    legend_patches = [mpatches.Patch(color=model_config.color(), label=model_config.name.DISPLAY_NAME) for model_config in model_configs]\n",
    "    for p in legend_patches:\n",
    "        p.set_edgecolor('black')\n",
    "    legend_patches.insert(0, Line2D([0], [0], color='black', lw=2, linestyle='--', label='Optimal Runtime'))\n",
    "    legend_patches.insert(9, plt.Line2D([], [], linewidth=0))\n",
    "    ax2.legend(handles=legend_patches, \n",
    "               loc='center right', \n",
    "               bbox_to_anchor=(-0.2, 0.5), \n",
    "               edgecolor='white', \n",
    "               labelspacing=0.1, \n",
    "               fontsize=fontsize * 0.8)\n",
    "    \n",
    "    #for i in ax2.containers:\n",
    "    #    ax2.bar_label(i,fmt='%.0f', label_type='edge', fontsize=fontsize*0.7)\n",
    "        \n",
    "    fig.align_labels()\n",
    "    plt.savefig(path, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e895ca24ff4871eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_configs = ACT_CARD_MODEL_CONFIGS\n",
    "\n",
    "spearman_df = pd.DataFrame()\n",
    "runtime_df = pd.DataFrame()\n",
    "missed_plans_df = pd.DataFrame()\n",
    "overest_df  = pd.DataFrame()\n",
    "underest_df = pd.DataFrame()\n",
    "q_error_df = pd.DataFrame()\n",
    "\n",
    "minimal_runtimes = []\n",
    "num_tables = []\n",
    "for workload in EvalWorkloads.FullJoinOrder.imdb:\n",
    "    results = get_model_results(workload, model_configs)\n",
    "    for model in model_configs:\n",
    "        model_results = results[results[\"model\"] == model.name.DISPLAY_NAME]\n",
    "        spearman_corr = SpearmanCorrelation().evaluate_metric(preds=model_results[\"prediction\"], \n",
    "                                                              labels=model_results['runtime'])\n",
    "        spearman_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = spearman_corr\n",
    "        q_error = QError().evaluate_metric(preds=model_results[\"prediction\"], labels=model_results['runtime'])\n",
    "        q_error_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = q_error\n",
    "        selected_runtime = SelectedRuntime().evaluate_metric(preds=model_results[\"prediction\"], labels=model_results['runtime'])\n",
    "        runtime_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = selected_runtime\n",
    "        missed_plans = MissedPlansFraction().evaluate_metric(preds=model_results[\"prediction\"], labels=model_results['runtime'])\n",
    "        missed_plans_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = missed_plans\n",
    "        overest = MaxOverestimation().evaluate_metric(preds=model_results[\"prediction\"], labels=model_results['runtime'])\n",
    "        overest_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = overest\n",
    "        underest = MaxUnderestimation().evaluate_metric(preds=model_results[\"prediction\"], labels=model_results['runtime'])\n",
    "        underest_df.loc[model.name.DISPLAY_NAME, workload.get_workload_name()] = underest\n",
    "    minimal_runtimes.append(model_results[\"label\"].min())\n",
    "    num_tables.append(workload.num_tables)\n",
    "        \n",
    "# Merge numtables to spearmans:\n",
    "spearman_df = spearman_df.T\n",
    "spearman_df['num_tables'] = num_tables\n",
    "\n",
    "q_error_df = q_error_df.T\n",
    "q_error_df['num_tables'] = num_tables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d16d8ed14814538",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Group dataframe by num tables and prepare the data for plotting\n",
    "# Assuming q_error_df is already defined\n",
    "# Remove rows that contain at least 4 NaN values\n",
    "q_error_df = q_error_df.dropna(thresh=4)\n",
    "\n",
    "# Group dataframe by num tables and calculate the median for each group\n",
    "grouped = q_error_df.groupby('num_tables')\n",
    "\n",
    "# Create a single DataFrame to store all the median data for plotting\n",
    "median_data = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.drop(columns='num_tables')\n",
    "    median_group = group.median().reset_index()\n",
    "    median_group.columns = ['Model', 'Q-Error']\n",
    "    median_group['num_tables'] = name\n",
    "    median_data.append(median_group)\n",
    "\n",
    "# Concatenate all the median data into one DataFrame\n",
    "median_data = pd.concat(median_data)\n",
    "\n",
    "spearman_df = spearman_df.dropna(thresh=4)\n",
    "\n",
    "# Group dataframe by num tables and calculate the median for each group\n",
    "spearman_df = spearman_df.groupby('num_tables')\n",
    "\n",
    "# Create a single DataFrame to store all the median data for plotting\n",
    "spearman_df_collection = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.drop(columns='num_tables')\n",
    "    median_group = group.median().reset_index()\n",
    "    median_group.columns = ['Model', 'Q-Error']\n",
    "    median_group['num_tables'] = name\n",
    "    spearman_df_collection.append(median_group)\n",
    "\n",
    "# Concatenate all the median data into one DataFrame\n",
    "spearman_df_collection = pd.concat(spearman_df_collection)\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "sns.barplot(data=median_data, x='num_tables', y='Q-Error', hue='Model', palette=ColorManager.COLOR_PALETTE, ci=None, ax=axs[0])\n",
    "sns.barplot(data=spearman_df_collection, x='num_tables', y='Q-Error', hue='Model', palette=ColorManager.COLOR_PALETTE, ax=axs[1])\n",
    "axs[0].set_ylim(1, 3.5)\n",
    "#ax.set_yscale('log')\n",
    "\n",
    "    \n",
    "# Create legend\n",
    "legend_patches = [mpatches.Patch(color=model_config.color(), label=model_config.name.DISPLAY_NAME) for model_config in model_configs]\n",
    "for p in legend_patches:\n",
    "    p.set_edgecolor('black')\n",
    "    \n",
    "ax.legend(handles=legend_patches, \n",
    "           loc='center right', \n",
    "           bbox_to_anchor=(-0.2, 0.5), \n",
    "           edgecolor='white', \n",
    "           labelspacing=0.3, \n",
    "           fontsize=fontsize)\n",
    "    \n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a0213e155aa6ba1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
